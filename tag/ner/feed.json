{
    "version": "https://jsonfeed.org/version/1",
    "title": "白骨生花 • All posts by \"ner\" tag",
    "description": "同行者，拿起火把！",
    "home_page_url": "http://qiji5211.com",
    "items": [
        {
            "id": "http://qiji5211.com/2024/02/07/NER/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/",
            "url": "http://qiji5211.com/2024/02/07/NER/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/",
            "title": "文本预处理",
            "date_published": "2024-02-07T09:30:58.000Z",
            "content_html": "<h1 id=\"文本处理的基本方法\"><a class=\"anchor\" href=\"#文本处理的基本方法\">#</a> 文本处理的基本方法</h1>\n<h2 id=\"分词\"><a class=\"anchor\" href=\"#分词\">#</a> 分词</h2>\n<h3 id=\"什么是分词\"><a class=\"anchor\" href=\"#什么是分词\">#</a> 什么是分词？</h3>\n<blockquote>\n<p>分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的<br />\n行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界<br />\n符来简单划界，唯独词没有一个形式上的分界符，分词过程就是找到这样分界符的过程，</p>\n</blockquote>\n<ul>\n<li>举例：</li>\n</ul>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>女干事每月经过下属科室都</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">==</span><span class=\"token operator\">></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token string\">'女干事'</span>，<span class=\"token string\">'每月'</span>，<span class=\"token string\">'经过'</span>，<span class=\"token string\">'下属'</span>，<span class=\"token string\">'科室'</span>，<span class=\"token string\">'都'</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><h3 id=\"分词的作用\"><a class=\"anchor\" href=\"#分词的作用\">#</a> 分词的作用</h3>\n<ul>\n<li>词作为语言语义理解的最小单元，是人类理解文本语言的基础。因此也是 AI 解决 NLP 领域高阶任务，如自动问答，机器翻译，文本生成的重要基础环节。</li>\n</ul>\n<h3 id=\"流行中文分词工具jieba\"><a class=\"anchor\" href=\"#流行中文分词工具jieba\">#</a> 流行中文分词工具 jieba</h3>\n<blockquote>\n<p>&quot;结巴&quot; 中文分词，做最好的 Python 中文分词组件。</p>\n</blockquote>\n<ul>\n<li>\n<p>jiebal 的特性：</p>\n<ul>\n<li>支持多种分词模式</li>\n<li>精确模式</li>\n<li>全模式</li>\n<li>搜索引擎摸式</li>\n<li>支持中文繁体分词</li>\n<li>支持用户自定义词典</li>\n</ul>\n</li>\n<li>\n<p>jieba 使用</p>\n</li>\n</ul>\n<blockquote>\n<p>精确模式分词：试图将句子最精确地切开，适合文本分析</p>\n</blockquote>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">import</span> jieba</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> content <span class=\"token operator\">=</span> <span class=\"token string\">\"女干事每月经过下属科室都\"</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>cut<span class=\"token punctuation\">(</span>content<span class=\"token punctuation\">,</span>cut_all<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>默认为<span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\">#生成器对象</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token operator\">&lt;</span>generator <span class=\"token builtin\">object</span> Tokenizer<span class=\"token punctuation\">.</span>cut at <span class=\"token number\">0x000001ED13237350</span><span class=\"token operator\">></span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>lcut<span class=\"token punctuation\">(</span>content<span class=\"token punctuation\">,</span>cut_all<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token comment\">#列表内容</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>Building prefix <span class=\"token builtin\">dict</span> <span class=\"token keyword\">from</span> the default dictionary <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>Dumping model to <span class=\"token builtin\">file</span> cache C<span class=\"token punctuation\">:</span>\\WINDOWS\\TEMP\\jieba<span class=\"token punctuation\">.</span>cache</pre></td></tr><tr><td data-num=\"13\"></td><td><pre>Loading model cost <span class=\"token number\">0.607</span> seconds<span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>Prefix <span class=\"token builtin\">dict</span> has been built successfully<span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>Out<span class=\"token punctuation\">[</span><span class=\"token number\">9</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'女干事'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'每月'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'经过'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'下属'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'科室'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'都'</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><blockquote>\n<p>全模式分词：把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能消除歧义。</p>\n</blockquote>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> content <span class=\"token operator\">=</span> <span class=\"token string\">\"女干事每月经过下属科室都\"</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>cut<span class=\"token punctuation\">(</span>content<span class=\"token punctuation\">,</span>cut_all<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre> <span class=\"token operator\">&lt;</span>generator <span class=\"token builtin\">object</span> Tokenizer<span class=\"token punctuation\">.</span>cut at <span class=\"token number\">0x000001ED13237D60</span><span class=\"token operator\">></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>lcut<span class=\"token punctuation\">(</span>content<span class=\"token punctuation\">,</span>cut_all<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre> <span class=\"token punctuation\">[</span><span class=\"token string\">'女干事'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'干事'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'每月'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'月经'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'经过'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'下属'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'科室'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'都'</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><blockquote>\n<p>搜索引擎模式分词：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</p>\n</blockquote>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> content <span class=\"token operator\">=</span> <span class=\"token string\">\"女干事每月经过下属科室都\"</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>cut_for_search<span class=\"token punctuation\">(</span>content<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre> <span class=\"token operator\">&lt;</span>generator <span class=\"token builtin\">object</span> Tokenizer<span class=\"token punctuation\">.</span>cut_for_search at <span class=\"token number\">0x000001ED14A014A0</span><span class=\"token operator\">></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>lcut_for_search<span class=\"token punctuation\">(</span>content<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token string\">'干事'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'女干事'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'每月'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'经过'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'下属'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'科室'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'都'</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><blockquote>\n<p>中文繁体分词：针对中国香港，台湾地区的繁体文本进行分词</p>\n</blockquote>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> content <span class=\"token operator\">=</span> <span class=\"token string\">\"烦惱即是菩提，我暂且不提\"</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>lcut_for_search<span class=\"token punctuation\">(</span>content<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre> <span class=\"token punctuation\">[</span><span class=\"token string\">'烦惱'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'即'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'是'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'菩提'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'，'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'我'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'暂且'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'不提'</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><blockquote>\n<p>使用用户自定义词典</p>\n</blockquote>\n<ul>\n<li>添加自定义词典后，jieba 能够准确识别词典中出现的词汇，提升整体的识别准确率，</li>\n<li>词典格式：每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开<br />\n顺序不可颠倒</li>\n<li>词典样式如下，具体词性含义请参照附录：jieba 词性对照表，将该词典存 userdict.txt, 方便之后加载使用。</li>\n</ul>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>云计算5n</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>李小福2nr</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>easy_install <span class=\"token number\">3</span> eng</pre></td></tr><tr><td data-num=\"4\"></td><td><pre>好用<span class=\"token number\">300</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>韩玉赏鉴3nZ</pre></td></tr><tr><td data-num=\"6\"></td><td><pre>八一双鹿3nz</pre></td></tr></table></figure><hr />\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token operator\">>></span> <span class=\"token keyword\">import</span> jieba</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> jieba<span class=\"token punctuation\">.</span>1cut<span class=\"token punctuation\">(</span><span class=\"token string\">\"八一双鹿更名为八一南昌篮球队！\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\">#没有使用用户自定义词典前的结果：</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">[</span><span class=\"token string\">'八一双鹿'</span>，<span class=\"token string\">'更名'</span>，<span class=\"token string\">'为'</span>，<span class=\"token string\">'八一'</span>，<span class=\"token string\">'南昌'</span>，<span class=\"token string\">'篮球队'</span>，<span class=\"token string\">'!'</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token operator\">>></span>jieba<span class=\"token punctuation\">.</span>load_userdict<span class=\"token punctuation\">(</span><span class=\"token string\">\"./userdict.txt\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token comment\">#使用了用户自定义词典后的结果：</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token string\">'八一双鹿'</span>，<span class=\"token string\">'更名'</span>，<span class=\"token string\">'为'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'八一'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'南昌'</span>，<span class=\"token string\">'篮球队'</span>，<span class=\"token string\">'!'</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><h2 id=\"命名实体识别\"><a class=\"anchor\" href=\"#命名实体识别\">#</a> 命名实体识别</h2>\n<p>顾名思义，命名实体识别 (Named Entity Recognition, 简称 NER) 就是识别出一段文本中可能存在的命名实体。</p>\n<ul>\n<li>命名实体识别的作用：</li>\n</ul>\n<p>同词汇一样，命名实体也是人类理解文本的基础单元，因此也是 A 解决 NLP 领域高阶任务的重要基础环节</p>\n<h2 id=\"词性标注\"><a class=\"anchor\" href=\"#词性标注\">#</a> 词性标注</h2>\n<blockquote>\n<p>词性：语言中对词的一种分类方法，以语法特征为主要依据、兼顾词汇意义对词进行划分的结果，常见的词性有 143 如：名词，动词，形容词等</p>\n</blockquote>\n<p>顾名思义，词性标注 (Part-Of-Speech tagging, 简称 POS) 就是标注出一段文本中每个词汇的词性。</p>\n<ul>\n<li>词性标注的作用</li>\n</ul>\n<p>词性标注以分词为基础，是对文本语言的另一个角度的理解，因此也常常成为 A 解决 NLP 领域高阶任务的重要基础环节。</p>\n<ul>\n<li>使用 jieba 进行中文词性标注</li>\n</ul>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">import</span> jieba<span class=\"token punctuation\">.</span>posseg <span class=\"token keyword\">as</span> pseg</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">>></span><span class=\"token operator\">></span> pseg<span class=\"token punctuation\">.</span>lcut<span class=\"token punctuation\">(</span><span class=\"token string\">\"我爱北京天安门\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">[</span>pair<span class=\"token punctuation\">(</span><span class=\"token string\">'我'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> pair<span class=\"token punctuation\">(</span><span class=\"token string\">'爱'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'v'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> pair<span class=\"token punctuation\">(</span><span class=\"token string\">'北京'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'ns'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> pair<span class=\"token punctuation\">(</span><span class=\"token string\">'天安门'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'ns'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>rr<span class=\"token punctuation\">:</span>人称代词</pre></td></tr><tr><td data-num=\"6\"></td><td><pre>v<span class=\"token punctuation\">:</span>动词</pre></td></tr><tr><td data-num=\"7\"></td><td><pre>n<span class=\"token punctuation\">:</span>名词</pre></td></tr><tr><td data-num=\"8\"></td><td><pre>vn<span class=\"token punctuation\">:</span>动名词</pre></td></tr></table></figure><h1 id=\"文本张量表示方法\"><a class=\"anchor\" href=\"#文本张量表示方法\">#</a> 文本张量表示方法</h1>\n<ul>\n<li>什么是文本张量表示</li>\n</ul>\n<p>将一段文本使用张量进行表示，其中一般将词汇为表示成向量，称作词向量，再由各个词向量按顺序组成矩阵形成文本表示。</p>\n<p>举个栗子</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token string\">\"人生\"</span>，<span class=\"token string\">\"该\"</span>，<span class=\"token string\">\"如何\"</span>，<span class=\"token string\">\"起头\"</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token operator\">==</span><span class=\"token operator\">></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\">#每个词对应矩阵中的一个向量</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1.32</span><span class=\"token punctuation\">,</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token number\">5.2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token number\">3.1</span><span class=\"token punctuation\">,</span><span class=\"token number\">5.43</span><span class=\"token punctuation\">,</span><span class=\"token number\">0.34</span><span class=\"token punctuation\">,</span><span class=\"token number\">3.2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token number\">3.21</span><span class=\"token punctuation\">,</span><span class=\"token number\">5.32</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">4.32</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token number\">2.54</span><span class=\"token punctuation\">,</span><span class=\"token number\">7.32</span><span class=\"token punctuation\">,</span><span class=\"token number\">5.12</span><span class=\"token punctuation\">,</span><span class=\"token number\">9.54</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><ul>\n<li>文本张量表示的作用</li>\n</ul>\n<p>将文本表示成张量（矩阵）形式，能够使语言文本可以作为计算机处理程序的输入，进行接下来一系列的解析工作。</p>\n<blockquote>\n<ul>\n<li>文本张量表示的方法：\n<ul>\n<li>one-hot 编码</li>\n<li>Word2vec</li>\n<li>Word Embedding</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<h2 id=\"one-hot独热编码\"><a class=\"anchor\" href=\"#one-hot独热编码\">#</a> one-hot (独热编码)</h2>\n<ul>\n<li>什么是 one-hot (独热编码) 词向量表示</li>\n</ul>\n<p>又称独热编码，将每个词表示成具有个元素的向量，这个词向量中只有一个元素是 1，其他元素都是 0，不同词汇元素为 0 的位置不同，其中的大小是整个语料中不同词汇的总数。</p>\n<p>举个栗子</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token string\">\"改变\"</span>，<span class=\"token string\">\"要\"</span>，<span class=\"token string\">\"如何\"</span>，<span class=\"token string\">\"起手\"</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token operator\">==</span><span class=\"token operator\">></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token punctuation\">[</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><ul>\n<li>实现</li>\n</ul>\n<ol>\n<li>进行 one-hot (独热编码)</li>\n</ol>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">#导入用于对象保存与加载的 job11b</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>externals <span class=\"token keyword\">import</span> joblib</pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\">#导入 keras 中的词汇映射器 Tokenizer</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> Tokenizer</pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\">#假定 vocab 为语料集所有不同词汇集合</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>vocab <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span><span class=\"token string\">\"周杰伦\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"陈奕迅\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"王力宏\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"李宗盛\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"吴亦凡\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"鹿晗\"</span><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\">#实例化一个词汇映射器对象</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>t <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span>num_words<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>char_level<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token comment\">#使用映射器拟合现有文本数据</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>t<span class=\"token punctuation\">.</span>fit_on_texts<span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre></pre></td></tr><tr><td data-num=\"12\"></td><td><pre><span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> vocab<span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    zero_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>    <span class=\"token comment\">#使用映射器转化现有文本数据，** 每个词汇对应从 1 开始的自然数</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token comment\">#返回样式如：[[2]]，取出其中的数字需要使用 [0][0]</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>    token_index <span class=\"token operator\">=</span> t<span class=\"token punctuation\">.</span>texts_to_sequences<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>token<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>    zero_list<span class=\"token punctuation\">[</span>token_index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">1</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">,</span><span class=\"token string\">\"的one-hot编码为：\"</span><span class=\"token punctuation\">,</span>zero_list<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>李宗盛 的one<span class=\"token operator\">-</span>hot编码为： <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>王力宏 的one<span class=\"token operator\">-</span>hot编码为： <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>鹿晗 的one<span class=\"token operator\">-</span>hot编码为： <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>吴亦凡 的one<span class=\"token operator\">-</span>hot编码为： <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>陈奕迅 的one<span class=\"token operator\">-</span>hot编码为： <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre>周杰伦 的one<span class=\"token operator\">-</span>hot编码为： <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre></pre></td></tr><tr><td data-num=\"27\"></td><td><pre><span class=\"token comment\">#使用 jobl1b 工具保存映射器，以便之后使用</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre>tokenizer_path <span class=\"token string\">\"./Tokenizer\"</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre>joblib<span class=\"token punctuation\">.</span>dump<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">,</span>tokenizer_path<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><div class=\"note danger\">\n<p>注意！！！ 务必用 conda 安装 tensorflow, 会自动匹配对应的版本，pip 会有不兼容问题。<br />\ncpu 的直接 <code>conda install tensorflow</code>  即可，不要指定版本，会找不到包。然后根据对应的报错进行 <code>conda install chardet</code> , 大功告成！</p>\n</div>\n<ol start=\"2\">\n<li>使用</li>\n</ol>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">#导入用于对象保存与加载的 job11b</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>externals <span class=\"token keyword\">import</span> joblib</pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\">#加载之前保存的 Tokenizer, 实例化一个 t 对象</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>t <span class=\"token operator\">=</span> joblib<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>tokenizer_path<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\">#编码 token 为 \"李宗盛\"</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>token<span class=\"token operator\">=</span><span class=\"token string\">\"李宗盛\"</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token comment\">#使用 t 获得 token-index</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>token_index <span class=\"token operator\">=</span> t<span class=\"token punctuation\">.</span>texts_to_sequences<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>token<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token comment\">#初始化一个 zero-11st</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>zero_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre><span class=\"token comment\">#令 zero-L1st 的对应索引为 1</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>zero_list<span class=\"token punctuation\">[</span>token_index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">1</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">,</span><span class=\"token string\">\"的one-hot编码为：\"</span>，zero_list<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><ol start=\"3\">\n<li>输出效果</li>\n</ol>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>李宗盛的one<span class=\"token operator\">-</span>hot编码为：<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><ol start=\"4\">\n<li>one-hot 编码的优劣势：</li>\n</ol>\n<p>优势：摄作简单，容易理解。</p>\n<p>劣势：完全割裂了词与词之间的联系，而且在大语料集下，每个向量的长度过大，占据大量内存。</p>\n<p><span class=\"label danger\">: 说明</span></p>\n<p>正因为 one-hot 编码明显的劣势，这种编码方式被应用的地方越来越少，取而代之的是<br />\n接下来我们要学习的稠密向量的表示方法 word2vec 和 word embedding。</p>\n<h2 id=\"word2vec\"><a class=\"anchor\" href=\"#word2vec\">#</a> Word2vec</h2>\n<p>是一种流行的将词汇表示成向量的无监督训练方法，该过程将构建神经网络模型，将网络参<br />\n数作为词汇的向量表示，它包含 CBOW 和 skipgram 两种训练模式。</p>\n<h3 id=\"cbowcontinuous-bag-of-words模式\"><a class=\"anchor\" href=\"#cbowcontinuous-bag-of-words模式\">#</a> CBOW (Continuous bag of words) 模式</h3>\n<p>给定一段用于训练的文本语料，再选定某段长度（窗口）作为研究对象，使用上下文词汇预测<br />\n目标词汇。</p>\n<p><img data-src=\"/jike/image/1.png\" alt=\"tu\" title=\"CBOW模式\" /></p>\n<p>图中窗口大小为 9，使用前后 4 个词汇对目标词汇进行预测。</p>\n<h4 id=\"word2vec过程\"><a class=\"anchor\" href=\"#word2vec过程\">#</a> Word2vec 过程</h4>\n<p>假设我们给定的训练语料只有一句话： <code>Hope can set you free</code>  (愿你自由成长)，窗口大小为<br />\n 3, 因此模型的第一个训练样本来自 <code>Hope can set</code> , 因为是 CBOW 模式，所以将使用 <code>Hope</code>  和<br />\n <code>set</code>  作为输入， <code>can</code>  作为输出，在模型训练时， <code>Hope</code> , <code>can</code> , <code>set</code>  等词汇都使用它们的 one-hot 编码。如图所示：每个 one-hot 编码的单词与各自的变换矩阵 (即参数矩阵 3x5, 这里的 3 是指最后得到的词向量维度) 相乘之后再相加，得到上下文表示矩阵 (3x1)。</p>\n<p><img data-src=\"/jike/image/2.png\" alt=\"tu\" title=\"CBOW模式\" /></p>\n<p>接着，将上下文表示矩阵与变换矩阵（参数矩阵 5x3, 所有的变换矩阵共享参数）相乘，得到 5x1 的结果矩阵，它将与我们真正的目标矩阵即 can 的 one-hot 编码矩阵 (5x1) 进行损失的计算，然后更新网络参数完成一次模型迭代。</p>\n<p><img data-src=\"/jike/image/3.png\" alt=\"tu\" title=\"CBOW模式\" /></p>\n",
            "tags": [
                "NER"
            ]
        }
    ]
}