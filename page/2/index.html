<!-- build time:Fri Mar 22 2024 20:42:34 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="白骨生花" href="http://qiji5211.com/rss.xml"><link rel="alternate" type="application/atom+xml" title="白骨生花" href="http://qiji5211.com/atom.xml"><link rel="alternate" type="application/json" title="白骨生花" href="http://qiji5211.com/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="http://qiji5211.com/page/2/"><title>Alive = 白骨生花 = 嘿，来啦来啦！</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><a href="/" class="logo" rel="start"><p class="artboard">Alive</p><h1 itemprop="name headline" class="title">白骨生花</h1></a><p class="meta" itemprop="description">= 嘿，来啦来啦！ =</p></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Alive</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://i.imgs.ovh/2023/12/03/whivv.webp"></li><li class="item" data-background-image="https://i.imgs.ovh/2023/12/03/wxIiN.webp"></li><li class="item" data-background-image="https://i.imgs.ovh/2023/12/03/wh5R3.webp"></li><li class="item" data-background-image="https://i.imgs.ovh/2023/12/03/wsu2D.webp"></li><li class="item" data-background-image="https://i.imgs.ovh/2023/12/03/wsTMd.webp"></li><li class="item" data-background-image="https://i.imgs.ovh/2023/12/03/wx670.webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="index wrap"><div class="segments posts"><article class="item"><div class="cover"><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/07%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%94%B9%E9%80%A0%E7%AE%80%E4%BB%8B%EF%BC%88%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%AF%8D%E5%90%91%E9%87%8F%EF%BC%89/" itemprop="url" title="预训练语言模型的下游任务改造简介（如何使用词向量）"><img data-src="https://i.imgs.ovh/2023/12/03/wht5I.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2024-02-07 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2024-02-07T17:30:58+08:00">2024-02-07</time></span></div><h3><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/07%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%94%B9%E9%80%A0%E7%AE%80%E4%BB%8B%EF%BC%88%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%AF%8D%E5%90%91%E9%87%8F%EF%BC%89/" itemprop="url" title="预训练语言模型的下游任务改造简介（如何使用词向量）">预训练语言模型的下游任务改造简介（如何使用词向量）</a></h3><div class="excerpt"># Word2Vec --》 是一个神经网络语言模型，其次他的主要任务是做（生成词向量，Q） ![image-20220614194418918](../../Library/Application Support/typora-user-images/image-20220614194418918.png) Word2Vec 模型是不是预训练模型？（是） 一定是 什么是预训练？ 给出两个任务 A 和 B，任务 A 已经做出了模型 A，任务 B 无法解决（通过使用模型 A，加快任务的解决） 给你一个 NLP 里面的任务，给一个问题 X（Ni+ck），给出一个回答...</div><div class="meta footer"><span><a href="/categories/NER/" itemprop="url" title="NER"><i class="ic i-flag"></i>NER</a></span></div><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/07%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%94%B9%E9%80%A0%E7%AE%80%E4%BB%8B%EF%BC%88%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%AF%8D%E5%90%91%E9%87%8F%EF%BC%89/" itemprop="url" title="预训练语言模型的下游任务改造简介（如何使用词向量）" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/Transformer/" itemprop="url" title="Transformer"><img data-src="https://i.imgs.ovh/2023/12/03/whRTK.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2024-02-07 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2024-02-07T17:30:58+08:00">2024-02-07</time></span></div><h3><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/Transformer/" itemprop="url" title="Transformer">Transformer</a></h3><div class="excerpt"># 框架概述 1000*0.04=40--&amp;gt;10 5000*0.04=200--&amp;gt;20 预训练 --》NNLM--》word2Vec--》ELMo--》Attention NLP 中预训练的目的，其实就是为了生成词向量 顺水推舟，transformer 其实就是 attention 的一个堆叠 从一个宏观的角度，去看 transformer 到底在干嘛，然后在细分，再作总结 总分总 seq2seq 一句话，一个视频 序列（编码器）到序列（解码器） 分成两部分，编码器和解码器 # 整体框架 &amp;lt;img...</div><div class="meta footer"><span><a href="/categories/NER/" itemprop="url" title="NER"><i class="ic i-flag"></i>NER</a></span></div><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/Transformer/" itemprop="url" title="Transformer" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention%20%EF%BC%89/" itemprop="url" title="注意力机制（Attention ）"><img data-src="https://i.imgs.ovh/2023/12/03/when2.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2024-02-07 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2024-02-07T17:30:58+08:00">2024-02-07</time></span></div><h3><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention%20%EF%BC%89/" itemprop="url" title="注意力机制（Attention ）">注意力机制（Attention ）</a></h3><div class="excerpt"># Attention（注意力机制） 你会注意什么？ 大数据（什么数据都有，重要的，不重要的） 对于重要的数据，我们要使用 对于不重要的数据，我们不太想使用 但是，对于一个模型而言（CNN、LSTM），很难决定什么重要，什么不重要 由此，注意力机制诞生了（有人发现了如何去在深度学习的模型上做注意力） 红色的是科学家们发现，如果给你一张这个图，你眼睛的重点会聚焦在红色区域 人 --》看脸 文章看标题 段落看开头 后面的落款 这些红色区域可能包含更多的信息，更重要的信息 注意力机制：我们会把我们的焦点聚焦在比较重要的事物上 # 怎么做注意力 我（查询对象 Q），这张图（被查询对象...</div><div class="meta footer"><span><a href="/categories/NER/" itemprop="url" title="NER"><i class="ic i-flag"></i>NER</a></span></div><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention%20%EF%BC%89/" itemprop="url" title="注意力机制（Attention ）" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/08%20ELMo%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8F%8C%E5%90%91LSTM%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E8%AF%8D%E5%90%91%E9%87%8F%E5%A4%9A%E4%B9%89%E9%97%AE%E9%A2%98%EF%BC%89/" itemprop="url" title="ELMo模型（双向LSTM模型解决词向量多义问题）"><img data-src="https://i.imgs.ovh/2023/12/03/whYT9.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2024-02-07 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2024-02-07T17:30:58+08:00">2024-02-07</time></span></div><h3><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/08%20ELMo%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8F%8C%E5%90%91LSTM%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E8%AF%8D%E5%90%91%E9%87%8F%E5%A4%9A%E4%B9%89%E9%97%AE%E9%A2%98%EF%BC%89/" itemprop="url" title="ELMo模型（双向LSTM模型解决词向量多义问题）">ELMo模型（双向LSTM模型解决词向量多义问题）</a></h3><div class="excerpt"># Word2Vec 模型 NNLM 模型（是不是在预测下一个词，副产品是词向量） Word2Vec 模型：专门做词向量 CBOW Skip-gram ![image-20220614193540503](../../Library/Application Support/typora-user-images/image-20220614193540503.png) apple，苹果， # ELMo elmo 解决多义词问题 ELMo（专门做词向量，通过预训练） 不只是训练一个 Q 矩阵，我还可以把这个次的上下文信息融入到这个 Q 矩阵中 左边的 LSTM 获取 E2...</div><div class="meta footer"><span><a href="/categories/NER/" itemprop="url" title="NER"><i class="ic i-flag"></i>NER</a></span></div><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/08%20ELMo%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8F%8C%E5%90%91LSTM%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E8%AF%8D%E5%90%91%E9%87%8F%E5%A4%9A%E4%B9%89%E9%97%AE%E9%A2%98%EF%BC%89/" itemprop="url" title="ELMo模型（双向LSTM模型解决词向量多义问题）" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2023/11/23/zhan/%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99%E7%9A%84%E6%90%AD%E5%BB%BA/" itemprop="url" title="关于本站的搭建"><img data-src="https://i.imgs.ovh/2023/12/03/wsMqd.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2023-11-23 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2023-11-23T17:30:58+08:00">2023-11-23</time></span></div><h3><a href="/2023/11/23/zhan/%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99%E7%9A%84%E6%90%AD%E5%BB%BA/" itemprop="url" title="关于本站的搭建">关于本站的搭建</a></h3><div class="excerpt"># 前言 本博客是基于 node.js 中的 hexo 快速搭建的资源信息共享站，旨在帮助同学们快速有效地查找资料，免于信息差的焦虑和在各大群聊的奔波。评本站所有资料都可采用直链形式下载，因此可跨设备随想随看。 评论系统和搜索功能是用一些免费的企业服务对接完成的，后续会加上自己的后端，后续诸多功能会继续完善和更新。 目前部署在了 github 和 gitee 的服务器上，可以通过网址直接进行访问 ​ 所有文章均采用 Markdown 语法编写 md 文件，再由渲染器渲染成 html 文件，即可以说每一遍文章其实都是一个 html 文件。 # hexo...</div><div class="meta footer"><span><a href="/categories/%E7%AB%99%E7%82%B9%E6%90%AD%E5%BB%BA/" itemprop="url" title="站点搭建"><i class="ic i-flag"></i>站点搭建</a></span></div><a href="/2023/11/23/zhan/%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99%E7%9A%84%E6%90%AD%E5%BB%BA/" itemprop="url" title="关于本站的搭建" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2023/11/23/zhan/Hexo%20%E4%B8%BB%E9%A2%98%20Shoka%20&%20multi-markdown-it%20%E6%B8%B2%E6%9F%93%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/" itemprop="url" title="Hexo 主题 Shoka &amp; multi-markdown-it 渲染器使用说明"><img data-src="https://i.imgs.ovh/2023/12/03/whgbj.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2023-11-23 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2023-11-23T17:30:58+08:00">2023-11-23</time></span></div><h3><a href="/2023/11/23/zhan/Hexo%20%E4%B8%BB%E9%A2%98%20Shoka%20&%20multi-markdown-it%20%E6%B8%B2%E6%9F%93%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/" itemprop="url" title="Hexo 主题 Shoka &amp; multi-markdown-it 渲染器使用说明">Hexo 主题 Shoka & multi-markdown-it 渲染器使用说明</a></h3><div class="excerpt">hexo-theme-shoka ：本博客自用的主题 hexo-renderer-multi-markdown-it：配套的 markdown 渲染器 已经支持 hexo 5。 本项目是完全开源的，也有做一些 example 示例，大家可以随便拿随便改。 但是很抱歉，博主我暂时不能提供更多的支持，这个写的乱七八糟的文档，暂时也没有时间把它写得更专业一些。 非常对不住大家！ 当前版本更新至 0.2.5，更新记录点此 🚀快速开始 - 💌依赖插件 - 📌基本配置 - 🌈界面显示 - 🦄特殊功能 # 设计缘由 前几年在 Bear 和 Evernote 上整理了大量笔记，非常喜欢 Bear...</div><div class="meta footer"><span><a href="/categories/%E7%AB%99%E7%82%B9%E6%90%AD%E5%BB%BA/" itemprop="url" title="站点搭建"><i class="ic i-flag"></i>站点搭建</a></span></div><a href="/2023/11/23/zhan/Hexo%20%E4%B8%BB%E9%A2%98%20Shoka%20&%20multi-markdown-it%20%E6%B8%B2%E6%9F%93%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/" itemprop="url" title="Hexo 主题 Shoka &amp; multi-markdown-it 渲染器使用说明" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2023/11/23/qianduan/TSs/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" title="TypeScript教程"><img data-src="https://i.imgs.ovh/2023/12/03/wxBxm.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2023-11-23 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2023-11-23T17:30:58+08:00">2023-11-23</time></span></div><h3><a href="/2023/11/23/qianduan/TSs/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" title="TypeScript教程">TypeScript教程</a></h3><div class="excerpt"># 第一章 快速入门 # TypeScript 简介 TypeScript 是 JavaScript 的超集。 它对 JS 进行了扩展，向 JS 中引入了类型的概念，并添加了许多新的特性。 TS 代码需要通过编译器编译为 JS，然后再交由 JS 解析器执行。 TS 完全兼容 JS，换言之，任何的 JS 代码都可以直接当成 JS 使用。 相较于 JS 而言，TS 拥有了静态类型，更加严格的语法，更强大的功能；TS 可以在代码执行前就完成代码的检查，减小了运行时异常的出现的几率；TS 代码可以编译为任意版本的 JS 代码，可有效解决不同 JS 运行环境的兼容问题；同样的功能，TS 的代码量要大于...</div><div class="meta footer"><span><a href="/categories/qianduan/TSs/" itemprop="url" title="TS"><i class="ic i-flag"></i>TS</a></span></div><a href="/2023/11/23/qianduan/TSs/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" title="TypeScript教程" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2023/11/23/qianduan/JS/JS%E9%AB%98%E7%BA%A7/" itemprop="url" title="JavaScript高级教程"><img data-src="https://i.imgs.ovh/2023/12/03/wsd6T.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2023-11-23 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2023-11-23T17:30:58+08:00">2023-11-23</time></span></div><h3><a href="/2023/11/23/qianduan/JS/JS%E9%AB%98%E7%BA%A7/" itemprop="url" title="JavaScript高级教程">JavaScript高级教程</a></h3><div class="excerpt"># 基础总结深入 # 数据类型的分类和判断 基本 (值) 类型 Number ----- 任意数值 -------- typeof String ----- 任意字符串 ------ typeof Boolean ---- true/false ----- typeof undefined --- undefined ----- typeof/=== null -------- null ---------- === 对象 (引用) 类型 Object ----- typeof/instanceof Array ------ instanceof Function ----...</div><div class="meta footer"><span><a href="/categories/qianduan/JS/" itemprop="url" title="JS"><i class="ic i-flag"></i>JS</a></span></div><a href="/2023/11/23/qianduan/JS/JS%E9%AB%98%E7%BA%A7/" itemprop="url" title="JavaScript高级教程" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2023/11/23/qianduan/JS/mo/%E6%A8%A1%E5%9D%97%E5%8C%96_%E7%AC%94%E8%AE%B0/" itemprop="url" title="JS模块化（简版）"><img data-src="https://i.imgs.ovh/2023/12/03/wxCPJ.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2023-11-23 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2023-11-23T17:30:58+08:00">2023-11-23</time></span></div><h3><a href="/2023/11/23/qianduan/JS/mo/%E6%A8%A1%E5%9D%97%E5%8C%96_%E7%AC%94%E8%AE%B0/" itemprop="url" title="JS模块化（简版）">JS模块化（简版）</a></h3><div class="excerpt"># JS 模块化 模块化的理解 什么是模块？ 将一个复杂的程序依据一定的规则 (规范) 封装成几个块 (文件), 并进行组合在一起 块的内部数据 / 实现是私有的，只是向外部暴露一些接口 (方法) 与外部其它模块通信 一个模块的组成 数据 ---&amp;gt; 内部的属性 操作数据的行为 ---&amp;gt; 内部的函数 模块化 编码时是按照模块一个一个编码的，整个项目就是一个模块化的项目 模块化的进化过程 全局 function 模式 : 编码：全局变量 / 函数 问题：污染全局命名空间，容易引起命名冲突 / 数据不安全 namespace 模式 : 编码：将数据...</div><div class="meta footer"><span><a href="/categories/qianduan/JS/mo/" itemprop="url" title="模块化"><i class="ic i-flag"></i>模块化</a></span></div><a href="/2023/11/23/qianduan/JS/mo/%E6%A8%A1%E5%9D%97%E5%8C%96_%E7%AC%94%E8%AE%B0/" itemprop="url" title="JS模块化（简版）" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2023/11/23/qianduan/JS/mo/1_%E6%A8%A1%E5%9D%97%E5%8C%96%E8%BF%9B%E5%8C%96%E5%8F%B2%E6%95%99%E7%A8%8B/" itemprop="url" title="JS模块化"><img data-src="https://i.imgs.ovh/2023/12/03/whoBp.webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2023-11-23 17:30:58"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2023-11-23T17:30:58+08:00">2023-11-23</time></span></div><h3><a href="/2023/11/23/qianduan/JS/mo/1_%E6%A8%A1%E5%9D%97%E5%8C%96%E8%BF%9B%E5%8C%96%E5%8F%B2%E6%95%99%E7%A8%8B/" itemprop="url" title="JS模块化">JS模块化</a></h3><div class="excerpt"># 模块化进化史教程 # 模块化进化史教程 全局 function 模式 module1.js//数据 let data = &#39;atguigu.com&#39; //操作数据的函数 function foo() &amp;#123; console.log(`foo() $&amp;#123;data&amp;#125;`) &amp;#125; function bar() &amp;#123; console.log(`bar() $&amp;#123;data&amp;#125;`) &amp;#125; module2.jslet data2 = &#39;other...</div><div class="meta footer"><span><a href="/categories/qianduan/JS/mo/" itemprop="url" title="模块化"><i class="ic i-flag"></i>模块化</a></span></div><a href="/2023/11/23/qianduan/JS/mo/1_%E6%A8%A1%E5%9D%97%E5%8C%96%E8%BF%9B%E5%8C%96%E5%8F%B2%E6%95%99%E7%A8%8B/" itemprop="url" title="JS模块化" class="btn">more...</a></div></article></div></div><nav class="pagination"><div class="inner"><a class="extend prev" rel="prev" href="/"><i class="ic i-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="ic i-angle-right" aria-label="下一页"></i></a></div></nav></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="奇冀" data-src="/images/avatar.jpg"><p class="name" itemprop="name">奇冀</p><div class="description" itemprop="description">同行者，拿起火把！</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">28</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">10</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">9</span> <span class="name">标签</span></a></div></nav><div class="social"><a href="https://qiji5211.com/yourname" title="https:&#x2F;&#x2F;qiji5211.com&#x2F;yourname" class="item github"><i class="ic i-github"></i></a> <span class="exturl item bilibili" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vMTUwOTU1MzIzOD9zcG1faWRfZnJvbT0zMzMuMTAwNy4wLjA=" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;1509553238?spm_id_from&#x3D;333.1007.0.0"><i class="ic i-bilibili"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS85Ni0xNy01Ni00Nw==" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;96-17-56-47"><i class="ic i-zhihu"></i></span> <span class="exturl item email" data-url="bWFpbHRvOjk3MjQ3Nzg2N0BxcS5jb20=" title="mailto:972477867@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/page/3/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/NER/" title="分类于 NER">NER</a></div><span><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/06%20Word2Vec%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%93%E9%97%A8%E5%81%9A%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%8CCBOW%E5%92%8CSkip-gram%EF%BC%89/" title="Word2Vec模型（第一个专门做词向量的模型，CBOW和Skip-gram）">Word2Vec模型（第一个专门做词向量的模型，CBOW和Skip-gram）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/NER/" title="分类于 NER">NER</a></div><span><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/08%20ELMo%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8F%8C%E5%90%91LSTM%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E8%AF%8D%E5%90%91%E9%87%8F%E5%A4%9A%E4%B9%89%E9%97%AE%E9%A2%98%EF%BC%89/" title="ELMo模型（双向LSTM模型解决词向量多义问题）">ELMo模型（双向LSTM模型解决词向量多义问题）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/qianduan/" title="分类于 前端学习">前端学习</a> <i class="ic i-angle-right"></i> <a href="/categories/qianduan/TSs/" title="分类于 TS">TS</a></div><span><a href="/2023/11/23/qianduan/TSs/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" title="TypeScript教程">TypeScript教程</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/jike/" title="分类于 计科资料">计科资料</a></div><span><a href="/2023/11/23/jike/%E5%A4%A7%E5%88%9B%E9%A1%B9%E7%9B%AE%E5%90%88%E9%9B%86/" title="大创项目合集">大创项目合集</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/NER/" title="分类于 NER">NER</a></div><span><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/04%20%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88n%E5%85%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%89/" title="统计语言模型（n元语言模型）">统计语言模型（n元语言模型）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/jike/" title="分类于 计科资料">计科资料</a></div><span><a href="/2023/11/23/jike/%E6%8F%90%E9%97%AE%E7%9A%84%E6%99%BA%E6%85%A7/" title="提问的智慧">提问的智慧</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/NER/" title="分类于 NER">NER</a></div><span><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/Transformer/" title="Transformer">Transformer</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/NER/" title="分类于 NER">NER</a></div><span><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/05%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81+%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E8%B5%B7%E6%BA%90%EF%BC%89/" title="神经网络语言模型（独热编码+词向量的起源）">神经网络语言模型（独热编码+词向量的起源）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/NER/" title="分类于 NER">NER</a></div><span><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/03%20%E4%BB%80%E4%B9%88%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88Transformer%20%E5%89%8D%E5%A5%8F%EF%BC%89/" title="什么是预训练（Transformer 前奏）">什么是预训练（Transformer 前奏）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/NER/" title="分类于 NER">NER</a></div><span><a href="/2024/02/07/NER/Pre-training-language-model-main/%E7%AC%AC%E4%B8%80%E7%AF%87%20Transformer%E3%80%81GPT%E3%80%81BERT%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/00%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E5%85%A8%E6%96%87%2024854%20%E4%B8%AA%E8%AF%8D%EF%BC%89/" title="预训练语言模型的前世今生">预训练语言模型的前世今生</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">奇冀 @ Alive</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">203k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">3:05</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"page/2/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->